{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0fa9a9b-a015-4fee-a30c-dcca0b83c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterZoneEnv:\n",
    "    \"\"\"\n",
    "    A 2D grid environment for a drone exploring a disaster zone.\n",
    "\n",
    "    Legend (internally stored as integers):\n",
    "        0 -> Empty cell\n",
    "        1 -> Obstacle\n",
    "        2 -> Survivor\n",
    "        3 -> Resource\n",
    "\n",
    "    'D' in a scenario array indicates the drone's starting position (only in predefined scenarios).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width=8,\n",
    "        height=8,\n",
    "        num_obstacles=5,\n",
    "        num_survivors=3,\n",
    "        num_resources=2,\n",
    "        initial_energy=20,\n",
    "        dynamic=False,\n",
    "        predefined_grid=None,\n",
    "        seed=None,                # Optional random seed\n",
    "        recharge_amount=None      # If None => full recharge upon resource, else partial\n",
    "    ):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_obstacles = num_obstacles\n",
    "        self.num_survivors = num_survivors\n",
    "        self.num_resources = num_resources\n",
    "        self.initial_energy = initial_energy\n",
    "        self.energy = initial_energy\n",
    "        self.dynamic = dynamic\n",
    "        self.seed = seed\n",
    "        self.recharge_amount = recharge_amount\n",
    "        self.dynamic_changes = 0\n",
    "\n",
    "        # Define possible actions - up, down, left, right\n",
    "        self.action_space = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (1, 0),   # down\n",
    "            2: (0, -1),  # left\n",
    "            3: (0, 1)    # right\n",
    "        }\n",
    "\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # If we have a predefined grid, load that; otherwise random layout\n",
    "        if predefined_grid is not None:\n",
    "            self.reset_with_scenario(predefined_grid)\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Create a random environment layout:\n",
    "        - obstacles, survivors, resources randomly placed\n",
    "        - random drone start\n",
    "        \"\"\"\n",
    "        self.grid = np.zeros((self.height, self.width), dtype=int)\n",
    "\n",
    "        # Place obstacles\n",
    "        for _ in range(self.num_obstacles):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 1\n",
    "\n",
    "        # Place survivors\n",
    "        for _ in range(self.num_survivors):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 2\n",
    "\n",
    "        # Place resources\n",
    "        for _ in range(self.num_resources):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 3\n",
    "\n",
    "        # Random start for the drone\n",
    "        self.drone_x, self.drone_y = self._get_random_empty_cell()\n",
    "        self.energy = self.initial_energy\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def reset_with_scenario(self, scenario):\n",
    "        \"\"\"\n",
    "        Load a predefined grid that may contain 'D' for the drone start.\n",
    "        Convert it into an integer grid internally.\n",
    "        \"\"\"\n",
    "        scenario = np.array(scenario, dtype=object)\n",
    "        self.height, self.width = scenario.shape\n",
    "\n",
    "        self.grid = np.zeros((self.height, self.width), dtype=int)\n",
    "\n",
    "        drone_positions = []\n",
    "\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                val = scenario[i, j]\n",
    "                if val == 'D':\n",
    "                    drone_positions.append((i, j))\n",
    "                    self.grid[i, j] = 0  # treat 'D' cell as empty\n",
    "                else:\n",
    "                    self.grid[i, j] = int(val)\n",
    "\n",
    "        if len(drone_positions) != 1:\n",
    "            raise ValueError(\"Scenario must contain exactly one 'D' for the drone's start.\")\n",
    "        self.drone_x, self.drone_y = drone_positions[0]\n",
    "        self.energy = self.initial_energy\n",
    "\n",
    "    def _get_random_empty_cell(self):\n",
    "        while True:\n",
    "            x = random.randint(0, self.height - 1)\n",
    "            y = random.randint(0, self.width - 1)\n",
    "            if self.grid[x, y] == 0:\n",
    "                return x, y\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Returns the current state, which includes:\n",
    "        - Drone's position (x, y)\n",
    "        - Drone's energy level\n",
    "        - Information about the cells up, down, left, and right of the drone\n",
    "\n",
    "        A* and Dijkstra can ignore self.around, but it is essential for Q-Learning\n",
    "        \"\"\"\n",
    "        up = self.grid[self.drone_x - 1, self.drone_y] if self.drone_x > 0 else -1\n",
    "        down = self.grid[self.drone_x + 1, self.drone_y] if self.drone_x < self.height - 1 else -1\n",
    "        left = self.grid[self.drone_x, self.drone_y - 1] if self.drone_y > 0 else -1\n",
    "        right = self.grid[self.drone_x, self.drone_y + 1] if self.drone_y < self.width - 1 else -1\n",
    "\n",
    "    \n",
    "        # Store surrounding information in a tuple\n",
    "        self.around = (up, down, left, right)\n",
    "    \n",
    "        # Return the full state\n",
    "        return (self.drone_x, self.drone_y, self.energy, self.around)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes a step in the environment.\n",
    "        \"\"\"\n",
    "        dx, dy = self.action_space[action]\n",
    "        new_x = self.drone_x + dx\n",
    "        new_y = self.drone_y + dy\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if not self._in_bounds(new_x, new_y):\n",
    "            reward -= 10  # Penalty for trying to move out of bounds\n",
    "        elif self.grid[new_x, new_y] == 1:  # Obstacle collision\n",
    "            reward -= 10\n",
    "        else:\n",
    "            # Valid move\n",
    "            self.drone_x, self.drone_y = new_x, new_y\n",
    "\n",
    "            if self.grid[new_x, new_y] == 0:\n",
    "                reward += 1  # Reward for moving to an empty cell\n",
    "            elif self.grid[new_x, new_y] == 2:\n",
    "                reward += 10  # Reward for rescuing a survivor\n",
    "                self.grid[new_x, new_y] = 0  # Remove survivor\n",
    "            elif self.grid[new_x, new_y] == 3:\n",
    "                reward += 5  # Reward for collecting a resource\n",
    "                self.energy += 5  # Add 5 energy when collecting a resource\n",
    "                self.grid[new_x, new_y] = 0  # Remove resource\n",
    "\n",
    "        # Energy cost per move\n",
    "        self.energy -= 1\n",
    "        reward -= 1  # Decrease reward for the energy spent moving\n",
    "\n",
    "        if self.energy <= 0:\n",
    "            done = True\n",
    "\n",
    "        return self._get_state(), reward, done\n",
    "    \n",
    "\n",
    "    def _in_bounds(self, x, y):\n",
    "        \"\"\"Check if the position is within the grid boundaries.\"\"\"\n",
    "        return 0 <= x < self.height and 0 <= y < self.width\n",
    "    \n",
    "    \n",
    "    def apply_dynamic_changes(self, step_count):\n",
    "        \"\"\"\n",
    "        Example dynamic changes:\n",
    "         - Add an obstacle every 5 steps\n",
    "         - Move all survivors every 3 steps\n",
    "        \"\"\"\n",
    "        if not self.dynamic:\n",
    "            return\n",
    "\n",
    "        # Add an obstacle every 5 steps\n",
    "        if step_count > 0 and step_count % 5 == 0:\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 1\n",
    "            self.dynamic_changes += 1\n",
    "\n",
    "        # Move survivors every 3 steps\n",
    "        if step_count > 0 and step_count % 3 == 0:\n",
    "            survivor_positions = [\n",
    "                (xx, yy) for xx in range(self.height)\n",
    "                for yy in range(self.width)\n",
    "                if self.grid[xx, yy] == 2\n",
    "            ]\n",
    "            for (sx, sy) in survivor_positions:\n",
    "                self.grid[sx, sy] = 0\n",
    "                nx, ny = self._get_random_empty_cell()\n",
    "                self.grid[nx, ny] = 2\n",
    "\n",
    "    def render(self):\n",
    "        grid_copy = self.grid.astype(str)\n",
    "        grid_copy[grid_copy == '0'] = '.'\n",
    "        grid_copy[grid_copy == '1'] = '#'\n",
    "        grid_copy[grid_copy == '2'] = 'S'\n",
    "        grid_copy[grid_copy == '3'] = 'R'\n",
    "\n",
    "        # Mark the drone location\n",
    "        grid_copy[self.drone_x, self.drone_y] = 'D'\n",
    "\n",
    "        for row in grid_copy:\n",
    "            print(\" \".join(row))\n",
    "        print(f\"Energy: {self.energy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639f6001-9af6-4373-9eab-93b49bebcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def initialize_q_table_dict(action_space):\n",
    "    \"\"\"\n",
    "    Initializes a Q-table using a dictionary to handle large state spaces efficiently.\n",
    "    :param action_space: The action space of the environment to determine the action space size.\n",
    "    :return: A defaultdict for the Q-table.\n",
    "    \"\"\"\n",
    "    return defaultdict(lambda: np.zeros(len(action_space)))\n",
    "\n",
    "def compute_state_key(around):\n",
    "    \"\"\"\n",
    "    Computes a unique state key based on the agent's surrounding grid.\n",
    "    :param around: Tuple containing information about up, down, left, and right cells.\n",
    "    :return: A hashable state key.\n",
    "    \"\"\"\n",
    "    return tuple(around)\n",
    "\n",
    "def q_learning_train_dict(env, q_table, episodes=10000, max_steps=100, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.996):\n",
    "    \"\"\"\n",
    "    Q-learning training loop for an agent with a simplified state structure (based on surroundings only).\n",
    "    Tracks average reward every 50,000 episodes and saves the Q-table.\n",
    "    \"\"\"\n",
    "    total_rewards = []  # Store rewards per episode for analysis\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()  # Reset the environment for a new episode\n",
    "        total_reward = 0\n",
    "        step_count = 0  # Step counter for dynamic changes\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Apply dynamic environment changes if applicable (before the agent perceives the state)\n",
    "            if env.dynamic:\n",
    "                env.apply_dynamic_changes(step_count)\n",
    "\n",
    "            # Unpack the state after dynamic changes\n",
    "            around = state[3]  # Only consider surroundings for the state key\n",
    "            state_key = compute_state_key(around)  # State as a tuple for Q-table indexing\n",
    "\n",
    "            # Choose action (epsilon-greedy)\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice(list(env.action_space.keys()))  # Exploration\n",
    "            else:\n",
    "                action = np.argmax(q_table[state_key])  # Exploitation\n",
    "\n",
    "            # Take the action and observe the next state, reward, and whether the episode is done\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Unpack the next state\n",
    "            next_around = next_state[3]\n",
    "            next_state_key = compute_state_key(next_around)\n",
    "\n",
    "            # Compute Q-values\n",
    "            current_q = q_table[state_key][action]\n",
    "            max_future_q = np.max(q_table[next_state_key])\n",
    "\n",
    "            # Update Q-value using the Bellman equation\n",
    "            q_table[state_key][action] = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
    "\n",
    "            # Update the state and accumulate the total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Increment step counter for dynamic changes\n",
    "            step_count += 1\n",
    "\n",
    "            # Break if the episode is done\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, 0.1)\n",
    "\n",
    "        # Track rewards\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        # Print average reward every 50,000 episodes\n",
    "        if (episode + 1) % 50000 == 0:\n",
    "            avg_reward = np.mean(total_rewards[-50000:])\n",
    "            print(f\"Episode {episode + 1}/{episodes}: Average Reward (Last 50000 Episodes): {avg_reward:.2f}\")\n",
    "\n",
    "    return total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce69ca9-0d15-4762-91a0-15fcca5dc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pretrained_agent(env, q_table, max_steps=100):\n",
    "    \"\"\"\n",
    "    Test the agent using a pre-trained Q-table for one episode, including dynamic changes.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0  # Track steps for dynamic changes\n",
    "    num_survivors = 0\n",
    "    num_resources = 0\n",
    "\n",
    "    print(\"\\nTesting the agent:\")\n",
    "    env.render()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Unpack the state\n",
    "        around = state[3]  # Only the surroundings are part of the state now\n",
    "        state_key = compute_state_key(around)  # Adjusted to match the updated state key\n",
    "\n",
    "        # Exploit the pre-trained Q-table\n",
    "        if state_key in q_table:\n",
    "            action = np.argmax(q_table[state_key])\n",
    "            print(f\"Q-Values: {q_table[state_key]}\")\n",
    "        else:\n",
    "            # Fallback in case state is not in Q-table (shouldn't happen if trained well)\n",
    "            action = np.random.choice(list(env.action_space.keys()))\n",
    "            print(\"State not found in Q-table. Taking random action.\")\n",
    "\n",
    "        # Determine the target cell based on the action\n",
    "        dx, dy = env.action_space[action]\n",
    "        target_x = env.drone_x + dx\n",
    "        target_y = env.drone_y + dy\n",
    "\n",
    "        # Check if the target cell contains a survivor or resource\n",
    "        if env._in_bounds(target_x, target_y):\n",
    "            target_value = env.grid[target_x, target_y]\n",
    "            if target_value == 2:  # Survivor\n",
    "                num_survivors += 1\n",
    "                print(f\"Rescued a survivor at ({target_x}, {target_y})! Total survivors rescued: {num_survivors}\")\n",
    "            elif target_value == 3:  # Resource\n",
    "                num_resources += 1\n",
    "                print(f\"Collected a resource at ({target_x}, {target_y})! Total resources collected: {num_resources}\")\n",
    "\n",
    "        # Take action and apply dynamic changes\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.apply_dynamic_changes(step_count)  # Apply dynamic environment changes if applicable\n",
    "\n",
    "        print(f\"\\nStep {step + 1}:\")\n",
    "        print(f\"Action Taken: {action} -> Moving to ({target_x}, {target_y})\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        env.render()\n",
    "\n",
    "        # Update state and accumulate reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Increment step count for dynamic changes\n",
    "        step_count += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"\\nEpisode finished after {step + 1} steps with total reward {total_reward}\")\n",
    "            break\n",
    "\n",
    "    if not done:\n",
    "        print(f\"\\nEpisode ended after {max_steps} steps with total reward {total_reward}\")\n",
    "\n",
    "    # Final Results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(f\"Steps Taken: {step_count}\")\n",
    "    print(f\"Total Survivors Rescued: {num_survivors}\")\n",
    "    print(f\"Total Resources Collected: {num_resources}\")\n",
    "\n",
    "    return {\n",
    "        \"total_reward\": total_reward,\n",
    "        \"steps_taken\": step_count,\n",
    "        \"survivors_rescued\": num_survivors,\n",
    "        \"resources_collected\": num_resources,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ca1ec98-cddb-4a42-a824-9539e0b9eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Q-learning agent in the static environment...\n",
      "Episode 10000/100000: Average Reward (Last 10000 Episodes): 12.74\n",
      "Episode 20000/100000: Average Reward (Last 10000 Episodes): 14.36\n",
      "Episode 30000/100000: Average Reward (Last 10000 Episodes): 14.05\n",
      "Episode 40000/100000: Average Reward (Last 10000 Episodes): 14.48\n",
      "Episode 50000/100000: Average Reward (Last 10000 Episodes): 14.63\n",
      "Episode 60000/100000: Average Reward (Last 10000 Episodes): 14.41\n",
      "Episode 70000/100000: Average Reward (Last 10000 Episodes): 14.54\n",
      "Episode 80000/100000: Average Reward (Last 10000 Episodes): 14.65\n",
      "Episode 90000/100000: Average Reward (Last 10000 Episodes): 14.28\n",
      "Episode 100000/100000: Average Reward (Last 10000 Episodes): 14.36\n",
      "Training the Q-learning agent in the dynamic environment...\n",
      "Episode 10000/100000: Average Reward (Last 10000 Episodes): 28.31\n",
      "Episode 20000/100000: Average Reward (Last 10000 Episodes): 29.15\n",
      "Episode 30000/100000: Average Reward (Last 10000 Episodes): 29.15\n",
      "Episode 40000/100000: Average Reward (Last 10000 Episodes): 29.55\n",
      "Episode 50000/100000: Average Reward (Last 10000 Episodes): 29.17\n",
      "Episode 60000/100000: Average Reward (Last 10000 Episodes): 29.22\n",
      "Episode 70000/100000: Average Reward (Last 10000 Episodes): 29.60\n",
      "Episode 80000/100000: Average Reward (Last 10000 Episodes): 29.50\n",
      "Episode 90000/100000: Average Reward (Last 10000 Episodes): 29.12\n",
      "Episode 100000/100000: Average Reward (Last 10000 Episodes): 29.57\n"
     ]
    }
   ],
   "source": [
    "# In your main function:\n",
    "if __name__ == \"__main__\":\n",
    "    # Global variable to hold the Q-table\n",
    "    q_table_global = None\n",
    "    n = 300000\n",
    "\n",
    "    # Initialize the environment and Q-table for training\n",
    "    env_dynamic = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=5, num_resources=5, initial_energy=25, dynamic=True)\n",
    "    env_static = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=5, num_resources=5, initial_energy=25, dynamic=False)\n",
    "    \n",
    "    # Initialize the Q-table (pass the action space size from the environment)\n",
    "    q_table = initialize_q_table_dict(env_static.action_space)  # Use static env's action space\n",
    "\n",
    "    # Train the agent in the static environment\n",
    "    print(\"Training the Q-learning agent in the static environment...\")\n",
    "    static_rewards = q_learning_train_dict(env_static, q_table, episodes=n, max_steps=100)\n",
    "\n",
    "    # Train the agent in the dynamic environment\n",
    "    print(\"Training the Q-learning agent in the dynamic environment...\")\n",
    "    dynamic_rewards = q_learning_train_dict(env_dynamic, q_table, episodes=n, max_steps=100)\n",
    "\n",
    "    # Store the trained Q-table in a global variable\n",
    "    q_table_global = q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fed4f3b-6494-4f18-9dee-f935af79f7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the agent:\n",
      ". S . S . . #\n",
      ". # . . . . .\n",
      ". . . S R . .\n",
      "S . D R . . #\n",
      ". . . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . .\n",
      "Energy: 20\n",
      "\n",
      "Q-Values: [10.56372338 10.45085949 10.16396966 14.84128719]\n",
      "Collected a resource at (3, 3)! Total resources collected: 1\n",
      "\n",
      "Step 1:\n",
      "Action Taken: 3 -> Moving to (3, 3)\n",
      "Reward: 4\n",
      ". S . S . . #\n",
      ". # . . . . .\n",
      ". . . S R . .\n",
      "S . . D . . #\n",
      ". . . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . .\n",
      "Energy: 24\n",
      "\n",
      "Q-Values: [18.07444602 10.97916863 11.22994294 11.98290707]\n",
      "Rescued a survivor at (2, 3)! Total survivors rescued: 1\n",
      "\n",
      "Step 2:\n",
      "Action Taken: 0 -> Moving to (2, 3)\n",
      "Reward: 9\n",
      ". S . S . . #\n",
      ". # . . . . .\n",
      ". . . D R . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . .\n",
      "Energy: 23\n",
      "\n",
      "Q-Values: [10.56372338 10.45085949 10.16396966 14.84128719]\n",
      "Collected a resource at (2, 4)! Total resources collected: 2\n",
      "\n",
      "Step 3:\n",
      "Action Taken: 3 -> Moving to (2, 4)\n",
      "Reward: 4\n",
      ". S . S . . #\n",
      ". # . . . . .\n",
      ". . . . D . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . .\n",
      "Energy: 27\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 4:\n",
      "Action Taken: 0 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . . D . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R S\n",
      ". . S . S . #\n",
      ". . . # . . .\n",
      "Energy: 26\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 5:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". . . . D . #\n",
      ". # . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R S\n",
      ". . S . S . #\n",
      ". . . # . . .\n",
      "Energy: 25\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 6:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . . D . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R S\n",
      ". . S . S . #\n",
      ". . . # . . #\n",
      "Energy: 24\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 7:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". . . . D . #\n",
      ". # . . . . .\n",
      "S . . . . . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . #\n",
      "Energy: 23\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 8:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". . . . S . #\n",
      ". # . . D . .\n",
      "S . . . . . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . #\n",
      "Energy: 22\n",
      "\n",
      "Q-Values: [18.07444602 10.97916863 11.22994294 11.98290707]\n",
      "Rescued a survivor at (0, 4)! Total survivors rescued: 2\n",
      "\n",
      "Step 9:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 9\n",
      ". . . . D . #\n",
      ". # . . . . .\n",
      "S . . . . . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . #\n",
      "Energy: 21\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 10:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      "S # . . D . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". S . . . R .\n",
      ". . . . . . #\n",
      ". . . # . . #\n",
      "Energy: 20\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 11:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". . . . D . #\n",
      "S # . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". S . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 19\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 12:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      "S # . . D . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". S . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 18\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 13:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". . . . D . #\n",
      ". # . . . S .\n",
      ". . . . . . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 17\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 14:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . . D S .\n",
      ". . . . . . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 16\n",
      "\n",
      "Q-Values: [11.63356604 11.12225355 10.14881468 18.33514498]\n",
      "Rescued a survivor at (1, 5)! Total survivors rescued: 3\n",
      "\n",
      "Step 15:\n",
      "Action Taken: 3 -> Moving to (1, 5)\n",
      "Reward: 9\n",
      ". . . . . . #\n",
      ". # . . . D .\n",
      ". . . . . . .\n",
      "S . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 15\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 16:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # . # . . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . S . #\n",
      ". . . # . . #\n",
      "Energy: 14\n",
      "\n",
      "Q-Values: [-1.28155581 11.32817212  9.41183643 -1.12509188]\n",
      "\n",
      "Step 17:\n",
      "Action Taken: 1 -> Moving to (1, 5)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . # . D .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . S . #\n",
      ". . . # . . #\n",
      "Energy: 13\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 18:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # . # . . .\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . S . #\n",
      ". . . # . . #\n",
      "Energy: 12\n",
      "\n",
      "Q-Values: [-1.28155581 11.32817212  9.41183643 -1.12509188]\n",
      "\n",
      "Step 19:\n",
      "Action Taken: 1 -> Moving to (1, 5)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . # . D .\n",
      ". S . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 11\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 20:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # . # . . .\n",
      ". S . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 10\n",
      "\n",
      "Q-Values: [-1.28155581 11.32817212  9.41183643 -1.12509188]\n",
      "\n",
      "Step 21:\n",
      "Action Taken: 1 -> Moving to (1, 5)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . # . D #\n",
      ". S . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 9\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 22:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # . # . . #\n",
      ". . S . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 8\n",
      "\n",
      "Q-Values: [-1.28155581 11.32817212  9.41183643 -1.12509188]\n",
      "\n",
      "Step 23:\n",
      "Action Taken: 1 -> Moving to (1, 5)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . # . D #\n",
      ". . S . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 7\n",
      "\n",
      "Q-Values: [11.35204858  9.9546827   9.99415547 -0.49062638]\n",
      "\n",
      "Step 24:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # . # . . #\n",
      ". . S . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [-1.28155581 11.32817212  9.41183643 -1.12509188]\n",
      "\n",
      "Step 25:\n",
      "Action Taken: 1 -> Moving to (1, 5)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # S # . D #\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # . . . . #\n",
      ". . . # . . #\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [11.35204858  9.9546827   9.99415547 -0.49062638]\n",
      "\n",
      "Step 26:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # S # . . #\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # # . . . #\n",
      ". . . # . . #\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [-1.28155581 11.32817212  9.41183643 -1.12509188]\n",
      "\n",
      "Step 27:\n",
      "Action Taken: 1 -> Moving to (1, 5)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # S # . D #\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R .\n",
      ". # # . . . #\n",
      ". . . # . . #\n",
      "Energy: 3\n",
      "\n",
      "Q-Values: [11.35204858  9.9546827   9.99415547 -0.49062638]\n",
      "\n",
      "Step 28:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # . # . . #\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R S\n",
      ". # # . . . #\n",
      ". . . # . . #\n",
      "Energy: 2\n",
      "\n",
      "Q-Values: [-1.28155581 11.32817212  9.41183643 -1.12509188]\n",
      "\n",
      "Step 29:\n",
      "Action Taken: 1 -> Moving to (1, 5)\n",
      "Reward: 0\n",
      ". . . . . . #\n",
      ". # . # . D #\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R S\n",
      ". # # . . . #\n",
      ". . . # . . #\n",
      "Energy: 1\n",
      "\n",
      "Q-Values: [11.35204858  9.9546827   9.99415547 -0.49062638]\n",
      "\n",
      "Step 30:\n",
      "Action Taken: 0 -> Moving to (0, 5)\n",
      "Reward: 0\n",
      ". . . . . D #\n",
      ". # . # . . #\n",
      ". . . . . . .\n",
      ". . . . . . #\n",
      ". . . . . R S\n",
      ". # # . . . #\n",
      ". . . # . . #\n",
      "Energy: 0\n",
      "\n",
      "\n",
      "Episode finished after 30 steps with total reward 35\n",
      "\n",
      "Final Results:\n",
      "Total Reward: 35\n",
      "Steps Taken: 30\n",
      "Total Survivors Rescued: 3\n",
      "Total Resources Collected: 2\n",
      "\n",
      "Testing the agent in the static environment:\n",
      "\n",
      "Testing the agent:\n",
      ". S . . . . . .\n",
      ". . . . R . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . D . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 20\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 1:\n",
      "Action Taken: 0 -> Moving to (3, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . R . . .\n",
      "# . . . . R . .\n",
      ". . . . D . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 19\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 2:\n",
      "Action Taken: 0 -> Moving to (2, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . R . . .\n",
      "# . . . D R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 18\n",
      "\n",
      "Q-Values: [15.06180403 10.70092865 10.88132035 13.92345609]\n",
      "Collected a resource at (1, 4)! Total resources collected: 1\n",
      "\n",
      "Step 3:\n",
      "Action Taken: 0 -> Moving to (1, 4)\n",
      "Reward: 4\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 22\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 4:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 21\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 5:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 20\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 6:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 19\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 7:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 18\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 8:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 17\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 9:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 16\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 10:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 15\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 11:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 14\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 12:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 13\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 13:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 12\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 14:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 11\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 15:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 10\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 16:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 9\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 17:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 8\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 18:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 7\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 19:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 20:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 21:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 22:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 3\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 23:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 2\n",
      "\n",
      "Q-Values: [10.88276737  9.67575176  9.71777149  9.72603672]\n",
      "\n",
      "Step 24:\n",
      "Action Taken: 0 -> Moving to (0, 4)\n",
      "Reward: 0\n",
      ". S . . D . . .\n",
      ". . . . . . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 1\n",
      "\n",
      "Q-Values: [-0.36390825 10.93204461  9.40926175  9.62942251]\n",
      "\n",
      "Step 25:\n",
      "Action Taken: 1 -> Moving to (1, 4)\n",
      "Reward: 0\n",
      ". S . . . . . .\n",
      ". . . . D . . .\n",
      "# . . . . R . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . S . .\n",
      "# # . . . S . .\n",
      ". # . . # . S .\n",
      "Energy: 0\n",
      "\n",
      "\n",
      "Episode finished after 25 steps with total reward 4\n",
      "\n",
      "Final Results:\n",
      "Total Reward: 4\n",
      "Steps Taken: 25\n",
      "Total Survivors Rescued: 0\n",
      "Total Resources Collected: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_reward': 4,\n",
       " 'steps_taken': 25,\n",
       " 'survivors_rescued': 0,\n",
       " 'resources_collected': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_dynamic2 = DisasterZoneEnv(width=7, height=7, num_obstacles=5, num_survivors=4, num_resources=3, initial_energy=20, dynamic=True)\n",
    "env_static2 = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=4, num_resources=2, initial_energy=20, dynamic=False)\n",
    "\n",
    "\n",
    "test_pretrained_agent(env_dynamic2, q_table_global, max_steps=100)\n",
    "\n",
    "print(\"\\nTesting the agent in the static environment:\")\n",
    "test_pretrained_agent(env_static2, q_table_global, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb40ff8f-ae33-4da4-87f6-278af9a53356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    An agent that uses a pre-trained Q-table to navigate the DisasterZoneEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, q_table=None):\n",
    "        self.env = env\n",
    "        self.q_table = q_table if q_table else q_table_global  # Use the provided Q-table or global variable\n",
    "        self.steps_taken = 0\n",
    "        self.survivors_rescued = 0\n",
    "        self.resources_collected = 0\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Executes the agent in the given environment using the pre-trained Q-table.\n",
    "        \"\"\"\n",
    "        state = self.env.reset()\n",
    "        self.steps_taken = 0\n",
    "        self.survivors_rescued = 0\n",
    "        self.resources_collected = 0\n",
    "\n",
    "        for step in range(self.env.initial_energy):\n",
    "            # Unpack state\n",
    "            energy, around = state[2], state[3]\n",
    "            state_key = tuple(around)  # The state key does not use energy\n",
    "\n",
    "            # Decide action based on Q-table\n",
    "            if state_key in self.q_table:\n",
    "                action = np.argmax(self.q_table[state_key])\n",
    "            else:\n",
    "                # Fallback to random action if state is unknown\n",
    "                action = np.random.choice(list(self.env.action_space.keys()))\n",
    "\n",
    "            # Determine target cell\n",
    "            dx, dy = self.env.action_space[action]\n",
    "            target_x = self.env.drone_x + dx\n",
    "            target_y = self.env.drone_y + dy\n",
    "\n",
    "            # Check the cell content\n",
    "            if self.env._in_bounds(target_x, target_y):\n",
    "                target_value = self.env.grid[target_x, target_y]\n",
    "                if target_value == 2:  # Survivor\n",
    "                    self.survivors_rescued += 1\n",
    "                elif target_value == 3:  # Resource\n",
    "                    self.resources_collected += 1\n",
    "\n",
    "            # Take action\n",
    "            next_state, _, done = self.env.step(action)\n",
    "\n",
    "            # Update state and step count\n",
    "            state = next_state\n",
    "            self.steps_taken += 1\n",
    "\n",
    "            # Handle dynamic environment changes\n",
    "            if self.env.dynamic:\n",
    "                self.env.apply_dynamic_changes(self.steps_taken)\n",
    "\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "724039ad-852b-4da6-91a7-2b2234f4e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scenarios Dictionary\n",
    "\n",
    "SCENARIOS_DICT = {\n",
    "    \"Scenario_1\": {\n",
    "        \"name\": \"Simple Layout\",\n",
    "        \"grid\": np.array([\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            [0, 2, 0, 1, 0, 0, 3, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 2, 0, 0],\n",
    "            [3, 0, 0, 0, 0, 0, 0, 'D']\n",
    "        ], dtype=object),\n",
    "        \"dynamic\": False,\n",
    "        \"description\": \"Minimal obstacles, static environment\"\n",
    "    },\n",
    "    \"Scenario_2\": {\n",
    "        \"name\": \"Obstacle Maze\",\n",
    "        \"grid\": np.array([\n",
    "            [0, 1, 1, 1, 0, 0, 0, 0],\n",
    "            [0, 1, 2, 1, 0, 1, 1, 0],\n",
    "            [0, 1, 0, 1, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 1, 0, 0],\n",
    "            [0, 1, 0, 1, 0, 1, 0, 3],\n",
    "            [0, 1, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 2, 0],\n",
    "            ['D', 1, 0, 0, 0, 0, 0, 0]\n",
    "        ], dtype=object),\n",
    "        \"dynamic\": False,\n",
    "        \"description\": \"A more complex layout\"\n",
    "    },\n",
    "    \"Scenario_3\": {\n",
    "        \"name\": \"Dynamic Mixed Layout\",\n",
    "        \"grid\": np.array([\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 2, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 3, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 2, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 3, 0, 0, 0, 0],\n",
    "            ['D', 0, 0, 0, 0, 0, 0, 0]\n",
    "        ], dtype=object),\n",
    "        \"dynamic\": True,\n",
    "        \"description\": \"Dynamic environment with obstacles and survivors moving\"\n",
    "    },\n",
    "\n",
    "    # Larger 12x12 static\n",
    "    \"Scenario_4\": {\n",
    "        \"name\": \"Large 12x12 Static\",\n",
    "        \"grid\": np.array([\n",
    "            [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 2, 1, 1, 0, 0, 1, 0, 0, 3, 0, 0],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 2, 1, 0, 3, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [3, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0],\n",
    "            ['D', 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "        ], dtype=object),\n",
    "        \"dynamic\": False,\n",
    "        \"description\": \"A bigger static scenario with more obstacles, survivors, resources\"\n",
    "    },\n",
    "\n",
    "    # Larger 12x12 dynamic\n",
    "    \"Scenario_5\": {\n",
    "        \"name\": \"Large 12x12 Dynamic\",\n",
    "        \"grid\": np.array([\n",
    "            [0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0],\n",
    "            [0, 2, 1, 1, 0, 0, 1, 0, 0, 3, 0, 0],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 2, 1, 0, 3, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [3, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0],\n",
    "            ['D', 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "        ], dtype=object),\n",
    "        \"dynamic\": True,\n",
    "        \"description\": \"A bigger dynamic scenario for differences between A* and Dijkstra\"\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de20f7c8-a4ca-43a3-839d-2b0c2bbd9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scenario ID         Scenario Name  \\\n",
      "0  Scenario_1         Simple Layout   \n",
      "1  Scenario_2         Obstacle Maze   \n",
      "2  Scenario_3  Dynamic Mixed Layout   \n",
      "3  Scenario_4    Large 12x12 Static   \n",
      "4  Scenario_5   Large 12x12 Dynamic   \n",
      "\n",
      "                                         Description  Is Dynamic  \\\n",
      "0              Minimal obstacles, static environment       False   \n",
      "1                              A more complex layout       False   \n",
      "2  Dynamic environment with obstacles and survivo...        True   \n",
      "3  A bigger static scenario with more obstacles, ...       False   \n",
      "4  A bigger dynamic scenario for differences betw...        True   \n",
      "\n",
      "       Agent Name  Steps Taken  Survivors Rescued  Resources Collected  \\\n",
      "0  QLearningAgent           25                  2                    0   \n",
      "1  QLearningAgent           25                  2                    0   \n",
      "2  QLearningAgent           25                  3                    0   \n",
      "3  QLearningAgent           25                  0                    0   \n",
      "4  QLearningAgent           25                  1                    0   \n",
      "\n",
      "   Energy Used  Computation Time (s)  \n",
      "0           25              0.000160  \n",
      "1           25              0.000102  \n",
      "2           25              0.000203  \n",
      "3           25              0.000094  \n",
      "4           25              0.000304  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Cell 5: Tester\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    Runs multiple agents on multiple scenarios, collects metrics into a results table.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent_classes, scenarios_dict, env_params=None):\n",
    "        self.agent_classes = agent_classes\n",
    "        self.scenarios_dict = scenarios_dict\n",
    "        self.env_params = env_params if env_params else {}\n",
    "        self.results = []\n",
    "\n",
    "    def run_all_scenarios(self):\n",
    "        for scenario_id, scenario_info in self.scenarios_dict.items():\n",
    "            scenario_grid = scenario_info[\"grid\"]\n",
    "            scenario_name = scenario_info.get(\"name\", scenario_id)\n",
    "            scenario_dynamic = scenario_info.get(\"dynamic\", False)\n",
    "            scenario_desc = scenario_info.get(\"description\", \"\")\n",
    "\n",
    "            # Merge scenario-level dynamic with global env_params\n",
    "            # i.e., if scenario_dynamic is True, we override env_params' dynamic\n",
    "            param_dynamic = scenario_dynamic or self.env_params.get(\"dynamic\", False)\n",
    "            param_initial_energy = scenario_info.get(\"initial_energy\", self.env_params.get(\"initial_energy\", 20))\n",
    "\n",
    "            # Optionally override other parameters if needed (like num_obstacles, seed, etc.)\n",
    "            # but for now we just keep global env_params for these\n",
    "\n",
    "            for agent_class in self.agent_classes:\n",
    "                # Create environment\n",
    "                env = DisasterZoneEnv(\n",
    "                    width=self.env_params.get(\"width\", 8),\n",
    "                    height=self.env_params.get(\"height\", 8),\n",
    "                    num_obstacles=self.env_params.get(\"num_obstacles\", 5),\n",
    "                    num_survivors=self.env_params.get(\"num_survivors\", 3),\n",
    "                    num_resources=self.env_params.get(\"num_resources\", 2),\n",
    "                    initial_energy=param_initial_energy,\n",
    "                    dynamic=param_dynamic,\n",
    "                    predefined_grid=scenario_grid,\n",
    "                    seed=self.env_params.get(\"seed\", None),\n",
    "                    recharge_amount=self.env_params.get(\"recharge_amount\", None)\n",
    "                )\n",
    "\n",
    "                # Instantiate agent\n",
    "                agent = agent_class(env)\n",
    "\n",
    "                start_time = time.time()\n",
    "                agent.execute()\n",
    "                end_time = time.time()\n",
    "                comp_time = end_time - start_time\n",
    "\n",
    "                # Collect results\n",
    "                self.results.append({\n",
    "                    \"Scenario ID\": scenario_id,\n",
    "                    \"Scenario Name\": scenario_name,\n",
    "                    \"Description\": scenario_desc,\n",
    "                    \"Is Dynamic\": param_dynamic,\n",
    "                    \"Agent Name\": agent_class.__name__,\n",
    "                    \"Steps Taken\": getattr(agent, \"steps_taken\", None),\n",
    "                    \"Survivors Rescued\": getattr(agent, \"survivors_rescued\", None),\n",
    "                    \"Resources Collected\": getattr(agent, \"resources_collected\", None),\n",
    "                    \"Energy Used\": param_initial_energy - env.energy,\n",
    "                    \"Computation Time (s)\": comp_time\n",
    "                })\n",
    "\n",
    "    def get_results_df(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "    def save_results(self, filename=\"results.csv\"):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "\n",
    "# We'll compare Dijkstra and AStar on all scenarios\n",
    "agent_list = [QLearningAgent]\n",
    "\n",
    "# We'll test them with these default environment params\n",
    "env_params = {\n",
    "    \"width\": 12,           # We'll force a 12x12 grid for all, but scenario_1,2,3 are actually 8x8\n",
    "                           # The scenario's shape actually overrides this for predefined grids, but that's okay.\n",
    "    \"height\": 12,\n",
    "    \"num_obstacles\": 5,    # For random scenario usage only\n",
    "    \"num_survivors\": 3,\n",
    "    \"num_resources\": 2,\n",
    "    \"initial_energy\": 25,  # Slightly higher energy to handle bigger grids\n",
    "    \"dynamic\": False,      # We'll rely on scenario dict for dynamic= True or False\n",
    "    \"seed\": 42             # Reproducible\n",
    "}\n",
    "\n",
    "# Instantiate the tester\n",
    "tester = Tester(agent_classes=agent_list, scenarios_dict=SCENARIOS_DICT, env_params=env_params)\n",
    "\n",
    "# Run all scenarios\n",
    "tester.run_all_scenarios()\n",
    "\n",
    "# Get results as DataFrame\n",
    "df_results = tester.get_results_df()\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
