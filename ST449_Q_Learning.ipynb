{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827c3a36-e504-4d8a-80d4-34bf1ffb4b98",
   "metadata": {},
   "source": [
    "### How the Agent Works\n",
    "\n",
    "The drone is controlled using **Q-learning**, a type of reinforcement learning. This method lets the agent learn behaviors by interacting with the environment and receiving rewards. At the start, the agent knows nothing about the environment, but it learns over time by trying actions, getting feedback, and updating its knowledge.\n",
    "\n",
    "#### Decision-Making Through the Q-Table\n",
    "\n",
    "The agent uses a **Q-table** to guide its decisions. The Q-table is like a reference sheet where the agent stores information about the best actions to take in different situations (states). Here’s how it works:\n",
    "\n",
    "- **Exploration and Exploitation**: At first, the drone explores by trying random actions. Over time, it uses its Q-table to choose actions that are known to give the best rewards.\n",
    "- **State-Action Match**: The Q-table links each state (e.g., the drone’s position, energy level, and surroundings) to the possible actions (up, down, left, right). The agent checks the Q-values for each action and picks the one with the highest value.\n",
    "- **Learning the Best Behavior**: After every move, the agent updates the Q-table based on the rewards it gets. The more the agent trains, the better it becomes at choosing actions.\n",
    "\n",
    "#### Why This Algorithm is Realistic\n",
    "\n",
    "Unlike deterministic algorithms like **A*** or **Dijkstra**, where the full map is given and static, the Q-learning agent doesn't rely on having complete information. Instead, it explores and adapts. In a **dynamic environment**, where survivors move, obstacles appear, or resources are replenished, deterministic algorithms fail because they can't adapt to these changes. \n",
    "\n",
    "This is why Q-learning is more realistic. It’s designed to handle environments that change unpredictably, like a real disaster zone. However, because the Q-learning agent doesn’t know the full map and relies on exploration, it will be slower and less efficient in static, large maps compared to algorithms like A* or Dijkstra. But in dynamic environments, the Q-learning agent’s ability to adapt makes it a better choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc95658-c929-409d-89c0-79ec389ae5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DisasterZoneEnv:\n",
    "    \"\"\"\n",
    "    A simplified 2D grid environment for a drone exploring a disaster zone.\n",
    "\n",
    "    Legend:\n",
    "      0 -> Empty cell\n",
    "      1 -> Obstacle\n",
    "      2 -> Survivor\n",
    "      3 -> Resource\n",
    "      D -> Drone (tracked separately, but displayed in render)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, width=8, height=8, num_obstacles=5, num_survivors=3, num_resources=2, initial_energy=20, dynamic=False):\n",
    "        \"\"\"\n",
    "        Initialize the environment with configurable dimensions and grid contents.\n",
    "\n",
    "        :param width: Width of the grid.\n",
    "        :param height: Height of the grid.\n",
    "        :param num_obstacles: Number of obstacles to place in the grid.\n",
    "        :param num_survivors: Number of survivors to place in the grid.\n",
    "        :param num_resources: Number of resources to place in the grid.\n",
    "        :param initial_energy: Initial energy level of the drone.\n",
    "        :param dynamic: Whether the environment is dynamic (changes during simulation).\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_obstacles = num_obstacles\n",
    "        self.num_survivors = num_survivors\n",
    "        self.num_resources = num_resources\n",
    "        self.initial_energy = initial_energy\n",
    "        self.dynamic = dynamic  # Enable or disable dynamic changes\n",
    "\n",
    "        # Define possible actions - up, down, left, right\n",
    "        self.action_space = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (1, 0),   # down\n",
    "            2: (0, -1),  # left\n",
    "            3: (0, 1)    # right\n",
    "        }\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to a starting state.\n",
    "        \"\"\"\n",
    "        self.grid = np.zeros((self.height, self.width), dtype=int)\n",
    "\n",
    "        # Place obstacles randomly\n",
    "        for _ in range(self.num_obstacles):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 1\n",
    "\n",
    "        # Place survivors randomly\n",
    "        for _ in range(self.num_survivors):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 2\n",
    "\n",
    "        # Place resources randomly\n",
    "        for _ in range(self.num_resources):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 3\n",
    "\n",
    "        # Drone's starting position\n",
    "        self.drone_x, self.drone_y = self._get_random_empty_cell()\n",
    "        self.energy = self.initial_energy\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_random_empty_cell(self):\n",
    "        \"\"\"\n",
    "        Finds a random empty cell (not an obstacle, survivor, or resource).\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            x = random.randint(0, self.height - 1)\n",
    "            y = random.randint(0, self.width - 1)\n",
    "            if self.grid[x, y] == 0:\n",
    "                return x, y\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Returns the current state, which includes:\n",
    "        - Drone's position (x, y)\n",
    "        - Drone's energy level\n",
    "        - Information about the cells up, down, left, and right of the drone\n",
    "\n",
    "        A* and Dijkstra can ignore self.around, but it is essential for Q-Learning\n",
    "        \"\"\"\n",
    "        up = self.grid[self.drone_x - 1, self.drone_y] if self.drone_x > 0 else -1\n",
    "        down = self.grid[self.drone_x + 1, self.drone_y] if self.drone_x < self.height - 1 else -1\n",
    "        left = self.grid[self.drone_x, self.drone_y - 1] if self.drone_y > 0 else -1\n",
    "        right = self.grid[self.drone_x, self.drone_y + 1] if self.drone_y < self.width - 1 else -1\n",
    "\n",
    "    \n",
    "        # Store surrounding information in a tuple\n",
    "        self.around = (up, down, left, right)\n",
    "    \n",
    "        # Return the full state\n",
    "        return (self.drone_x, self.drone_y, self.energy, self.around)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes a step in the environment.\n",
    "        \"\"\"\n",
    "        dx, dy = self.action_space[action]\n",
    "        new_x = self.drone_x + dx\n",
    "        new_y = self.drone_y + dy\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if not self._in_bounds(new_x, new_y):\n",
    "            reward -= 10  # Penalty for trying to move out of bounds\n",
    "        elif self.grid[new_x, new_y] == 1:  # Obstacle collision\n",
    "            reward -= 10\n",
    "        else:\n",
    "            # Valid move\n",
    "            self.drone_x, self.drone_y = new_x, new_y\n",
    "\n",
    "            if self.grid[new_x, new_y] == 0:\n",
    "                reward += 1  # Reward for moving to an empty cell\n",
    "            elif self.grid[new_x, new_y] == 2:\n",
    "                reward += 10  # Reward for rescuing a survivor\n",
    "                self.grid[new_x, new_y] = 0  # Remove survivor\n",
    "            elif self.grid[new_x, new_y] == 3:\n",
    "                reward += 5  # Reward for collecting a resource\n",
    "                self.energy += 5  # Add 5 energy when collecting a resource\n",
    "                self.grid[new_x, new_y] = 0  # Remove resource\n",
    "\n",
    "        # Energy cost per move\n",
    "        self.energy -= 1\n",
    "        reward -= 1  # Decrease reward for the energy spent moving\n",
    "\n",
    "        if self.energy <= 0:\n",
    "            done = True\n",
    "\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "\n",
    "    def _in_bounds(self, x, y):\n",
    "        \"\"\"Check if the position is within the grid boundaries.\"\"\"\n",
    "        return 0 <= x < self.height and 0 <= y < self.width\n",
    "\n",
    "    def apply_dynamic_changes(self, step_count):\n",
    "        \"\"\"\n",
    "        Applies dynamic changes to the grid, such as adding obstacles, moving survivors,\n",
    "        and placing new resources, based on the current step count.\n",
    "\n",
    "        :param step_count: The current simulation step.\n",
    "        \"\"\"\n",
    "        if self.dynamic:\n",
    "            # Add a new obstacle every 5 steps\n",
    "            if step_count % 5 == 0:\n",
    "                x, y = self._get_random_empty_cell()\n",
    "                self.grid[x, y] = 1  # Add an obstacle\n",
    "                #print(f\"Dynamic Change: Added obstacle at ({x}, {y})\")\n",
    "\n",
    "            # Move survivors every 3 steps\n",
    "            if step_count % 3 == 0:\n",
    "                survivor_positions = [(x, y) for x in range(self.height)\n",
    "                                      for y in range(self.width) if self.grid[x, y] == 2]\n",
    "                for x, y in survivor_positions:\n",
    "                    self.grid[x, y] = 0  # Remove survivor from the current position\n",
    "                    new_x, new_y = self._get_random_empty_cell()\n",
    "                    self.grid[new_x, new_y] = 2  # Place survivor in a new position\n",
    "                    #print(f\"Dynamic Change: Moved survivor from ({x}, {y}) to ({new_x}, {new_y})\")\n",
    "\n",
    "            # Add a new resource every 7 steps\n",
    "            if step_count % 7 == 0:\n",
    "                x, y = self._get_random_empty_cell()\n",
    "                self.grid[x, y] = 3  # Add a resource\n",
    "                #print(f\"Dynamic Change: Added resource at ({x}, {y})\")      \n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the current environment state.\n",
    "        \"\"\"\n",
    "        grid_copy = self.grid.astype(str)\n",
    "        grid_copy[grid_copy == '0'] = '.'\n",
    "        grid_copy[grid_copy == '1'] = '#'\n",
    "        grid_copy[grid_copy == '2'] = 'S'\n",
    "        grid_copy[grid_copy == '3'] = 'R'\n",
    "        grid_copy[self.drone_x, self.drone_y] = 'D'\n",
    "\n",
    "        for row in grid_copy:\n",
    "            print(\" \".join(row))\n",
    "        print(f\"Energy: {self.energy}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a651a-18ec-4332-8700-9a3c74b19a3d",
   "metadata": {},
   "source": [
    "### Explanation of Functions\n",
    "\n",
    "#### `initialize_q_table_dict(action_space)`\n",
    "Initializes the Q-table as a dictionary, where each state is associated with a zeroed action value. This helps in handling large state spaces efficiently.\n",
    "\n",
    "#### `compute_state_key(energy, around)`\n",
    "Generates a unique key for each state based on the drone's energy level and its surroundings, allowing the Q-table to track state-action pairs effectively.\n",
    "\n",
    "#### `q_learning_train_dict(env, q_table, ...)`\n",
    "This function implements the Q-learning training loop. The core idea is to update the Q-values based on the **epsilon-greedy** algorithm:\n",
    "\n",
    "- **Epsilon-Greedy**: The agent chooses an action based on exploration or exploitation. With probability `epsilon`, the agent will explore and pick a random action; otherwise, it will exploit the Q-table by choosing the action with the highest value.\n",
    "- **Epsilon Decay**: Over time, `epsilon` decays, making the agent shift from exploration to more exploitation (using the knowledge it’s gained from its Q-table).\n",
    "  \n",
    "The exploration-exploitation balance is crucial for learning in unknown environments. The decay allows the agent to explore less as it becomes more confident in its actions.\n",
    "\n",
    "Other elements of the training loop involve updating the Q-values using the Bellman equation and tracking rewards to improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdeef91e-549d-4696-aa81-2718fbb3a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def initialize_q_table_dict(action_space):\n",
    "    \"\"\"\n",
    "    Initializes a Q-table using a dictionary to handle large state spaces efficiently.\n",
    "    :param action_space: The action space of the environment to determine the action space size.\n",
    "    :return: A defaultdict for the Q-table.\n",
    "    \"\"\"\n",
    "    return defaultdict(lambda: np.zeros(len(action_space)))\n",
    "    \n",
    "def compute_state_key(energy, around):\n",
    "    \"\"\"\n",
    "    Computes a unique state key based on the agent's energy and surrounding grid.\n",
    "    :param energy: Drone's current energy level.\n",
    "    :param around: Tuple containing information about up, down, left, and right cells.\n",
    "    :return: A hashable state key.\n",
    "    \"\"\"\n",
    "    return (energy, tuple(around))\n",
    "\n",
    "def q_learning_train_dict(env, q_table, episodes=10000, max_steps=100, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.996):\n",
    "    \"\"\"\n",
    "    Q-learning training loop for an agent with a simplified state structure (energy + surroundings).\n",
    "    Tracks average reward every 10,000 episodes and saves the Q-table.\n",
    "    \"\"\"\n",
    "    total_rewards = []  # Store rewards per episode for analysis\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()  # Reset the environment for a new episode\n",
    "        total_reward = 0\n",
    "        step_count = 0  # Step counter for dynamic changes\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Apply dynamic environment changes if applicable (this should happen *before* the agent perceives the state)\n",
    "            if env.dynamic:\n",
    "                env.apply_dynamic_changes(step_count)\n",
    "\n",
    "            # Unpack the state after dynamic changes\n",
    "            energy, around = state[2], state[3]\n",
    "            state_key = compute_state_key(energy, around)  # State as a tuple for Q-table indexing\n",
    "\n",
    "            # Choose action (epsilon-greedy)\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice(list(env.action_space.keys()))  # Exploration\n",
    "            else:\n",
    "                action = np.argmax(q_table[state_key])  # Exploitation\n",
    "\n",
    "            # Take the action and observe the next state, reward, and whether the episode is done\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Unpack the next state\n",
    "            next_energy, next_around = next_state[2], next_state[3]\n",
    "            next_state_key = compute_state_key(next_energy, next_around)\n",
    "\n",
    "            # Compute Q-values\n",
    "            current_q = q_table[state_key][action]\n",
    "            max_future_q = np.max(q_table[next_state_key])\n",
    "\n",
    "            # Update Q-value using the Bellman equation\n",
    "            q_table[state_key][action] = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
    "\n",
    "            # Update the state and accumulate the total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Increment step counter for dynamic changes\n",
    "            step_count += 1\n",
    "\n",
    "            # Break if the episode is done\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, 0.1)\n",
    "\n",
    "        # Track rewards\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        # Print average reward every 5,000 episodes\n",
    "        if (episode + 1) % 10000 == 0:\n",
    "            avg_reward = np.mean(total_rewards[-10000:])\n",
    "            print(f\"Episode {episode + 1}/{episodes}: Average Reward (Last 10000 Episodes): {avg_reward:.2f}\")\n",
    "\n",
    "    return total_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa82bb-1b42-42fe-aac7-b28198f678f5",
   "metadata": {},
   "source": [
    "### Testing the Pre-Trained Agent\n",
    "\n",
    "The `test_pretrained_agent` function is used to run a single experiment where the agent follows a pre-trained Q-table. It executes one episode, making decisions based on the learned values, and takes into account dynamic changes in the environment during the process. The function tracks the agent's steps, rewards, and prints useful information for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69224465-65ec-423a-aef2-85bb8a4b9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pretrained_agent(env, q_table, max_steps=100):\n",
    "    \"\"\"\n",
    "    Test the agent using a pre-trained Q-table for one episode, including dynamic changes.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0  # Track steps for dynamic changes\n",
    "\n",
    "    print(\"\\nTesting the agent:\")\n",
    "    env.render()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Unpack the state\n",
    "        energy, around = state[2], state[3]\n",
    "        state_key = (energy, tuple(around))  # Adjusted to match training state key\n",
    "\n",
    "        # Exploit the pre-trained Q-table\n",
    "        if state_key in q_table:\n",
    "            action = np.argmax(q_table[state_key])\n",
    "            print(f\"Q-Values: {q_table[state_key]}\")\n",
    "        else:\n",
    "            # Fallback in case state is not in Q-table (shouldn't happen if trained well)\n",
    "            action = np.random.choice(list(env.action_space.keys()))\n",
    "            print(\"State not found in Q-table. Taking random action.\")\n",
    "\n",
    "        # Print debug information\n",
    "        print(f\"\\nStep {step + 1}:\")\n",
    "        print(f\"Energy: {energy}\")\n",
    "        print(f\"Surrounding Information (around): {around}\")\n",
    "        print(f\"State: {state_key}\")\n",
    "        print(f\"Action Taken: {action}\")\n",
    "\n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.apply_dynamic_changes(step_count)  # Apply dynamic environment changes\n",
    "\n",
    "        print(f\"Reward: {reward}\")\n",
    "        env.render()\n",
    "\n",
    "        # Update state and accumulate reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Increment step count for dynamic changes\n",
    "        step_count += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"\\nEpisode finished after {step + 1} steps with total reward {total_reward}\")\n",
    "            break\n",
    "\n",
    "    if not done:\n",
    "        print(f\"\\nEpisode ended after {max_steps} steps with total reward {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4ac12-9903-473e-a308-ce9a3fcb3843",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "\n",
    "In this section, the agent is trained through two environments: a static one and a dynamic one. We begin by training the agent in the **static environment**, where the map remains unchanged throughout the episodes. This allows the Q-table to be updated based on consistent surroundings and helps the agent optimize its actions over time.\n",
    "\n",
    "After completing the training in the static environment, we move on to the **dynamic environment**, where the map changes randomly. For example, obstacles may appear, and survivors or resources may be relocated, making it more challenging for the agent to adapt. The Q-table continues to be updated during this phase, enabling the agent to learn how to navigate the dynamic changes more effectively.\n",
    "\n",
    "As the agent trains, the **reward typically improves** with each iteration, reflecting better decision-making. However, after many episodes, the improvement in reward becomes **smaller**, indicating that the agent is approaching optimal behavior for the given environment. This process is part of the agent's learning curve, where the updates to the Q-table become less significant as the agent stabilizes its strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cf127f-9125-4b37-b4ca-9a571314d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Q-learning agent in the static environment...\n",
      "Episode 10000/300000: Average Reward (Last 10000 Episodes): 6.42\n",
      "Episode 20000/300000: Average Reward (Last 10000 Episodes): 10.94\n",
      "Episode 30000/300000: Average Reward (Last 10000 Episodes): 11.72\n",
      "Episode 40000/300000: Average Reward (Last 10000 Episodes): 12.14\n",
      "Episode 50000/300000: Average Reward (Last 10000 Episodes): 12.10\n",
      "Episode 60000/300000: Average Reward (Last 10000 Episodes): 12.28\n",
      "Episode 70000/300000: Average Reward (Last 10000 Episodes): 12.67\n",
      "Episode 80000/300000: Average Reward (Last 10000 Episodes): 12.80\n",
      "Episode 90000/300000: Average Reward (Last 10000 Episodes): 12.57\n",
      "Episode 100000/300000: Average Reward (Last 10000 Episodes): 12.84\n",
      "Episode 110000/300000: Average Reward (Last 10000 Episodes): 12.83\n",
      "Episode 120000/300000: Average Reward (Last 10000 Episodes): 12.61\n",
      "Episode 130000/300000: Average Reward (Last 10000 Episodes): 13.00\n",
      "Episode 140000/300000: Average Reward (Last 10000 Episodes): 13.08\n",
      "Episode 150000/300000: Average Reward (Last 10000 Episodes): 12.96\n",
      "Episode 160000/300000: Average Reward (Last 10000 Episodes): 13.16\n",
      "Episode 170000/300000: Average Reward (Last 10000 Episodes): 13.28\n",
      "Episode 180000/300000: Average Reward (Last 10000 Episodes): 13.30\n",
      "Episode 190000/300000: Average Reward (Last 10000 Episodes): 13.22\n",
      "Episode 200000/300000: Average Reward (Last 10000 Episodes): 13.24\n",
      "Episode 210000/300000: Average Reward (Last 10000 Episodes): 13.14\n",
      "Episode 220000/300000: Average Reward (Last 10000 Episodes): 13.18\n",
      "Episode 230000/300000: Average Reward (Last 10000 Episodes): 13.45\n",
      "Episode 240000/300000: Average Reward (Last 10000 Episodes): 13.13\n",
      "Episode 250000/300000: Average Reward (Last 10000 Episodes): 13.33\n",
      "Episode 260000/300000: Average Reward (Last 10000 Episodes): 13.46\n",
      "Episode 270000/300000: Average Reward (Last 10000 Episodes): 13.40\n",
      "Episode 280000/300000: Average Reward (Last 10000 Episodes): 13.55\n",
      "Episode 290000/300000: Average Reward (Last 10000 Episodes): 13.75\n",
      "Episode 300000/300000: Average Reward (Last 10000 Episodes): 13.37\n",
      "Training the Q-learning agent in the dynamic environment...\n",
      "Episode 10000/300000: Average Reward (Last 10000 Episodes): 25.95\n",
      "Episode 20000/300000: Average Reward (Last 10000 Episodes): 27.12\n",
      "Episode 30000/300000: Average Reward (Last 10000 Episodes): 27.33\n",
      "Episode 40000/300000: Average Reward (Last 10000 Episodes): 27.46\n",
      "Episode 50000/300000: Average Reward (Last 10000 Episodes): 27.26\n",
      "Episode 60000/300000: Average Reward (Last 10000 Episodes): 27.47\n",
      "Episode 70000/300000: Average Reward (Last 10000 Episodes): 27.60\n",
      "Episode 80000/300000: Average Reward (Last 10000 Episodes): 27.37\n",
      "Episode 90000/300000: Average Reward (Last 10000 Episodes): 27.88\n",
      "Episode 100000/300000: Average Reward (Last 10000 Episodes): 27.63\n",
      "Episode 110000/300000: Average Reward (Last 10000 Episodes): 27.65\n",
      "Episode 120000/300000: Average Reward (Last 10000 Episodes): 27.55\n",
      "Episode 130000/300000: Average Reward (Last 10000 Episodes): 27.48\n",
      "Episode 140000/300000: Average Reward (Last 10000 Episodes): 27.81\n",
      "Episode 150000/300000: Average Reward (Last 10000 Episodes): 27.61\n",
      "Episode 160000/300000: Average Reward (Last 10000 Episodes): 27.79\n",
      "Episode 170000/300000: Average Reward (Last 10000 Episodes): 27.70\n",
      "Episode 180000/300000: Average Reward (Last 10000 Episodes): 28.03\n",
      "Episode 190000/300000: Average Reward (Last 10000 Episodes): 27.91\n",
      "Episode 200000/300000: Average Reward (Last 10000 Episodes): 27.99\n",
      "Episode 210000/300000: Average Reward (Last 10000 Episodes): 28.07\n",
      "Episode 220000/300000: Average Reward (Last 10000 Episodes): 27.92\n",
      "Episode 230000/300000: Average Reward (Last 10000 Episodes): 27.86\n",
      "Episode 240000/300000: Average Reward (Last 10000 Episodes): 27.94\n",
      "Episode 250000/300000: Average Reward (Last 10000 Episodes): 27.84\n",
      "Episode 260000/300000: Average Reward (Last 10000 Episodes): 28.18\n",
      "Episode 270000/300000: Average Reward (Last 10000 Episodes): 28.11\n",
      "Episode 280000/300000: Average Reward (Last 10000 Episodes): 28.05\n",
      "Episode 290000/300000: Average Reward (Last 10000 Episodes): 28.20\n",
      "Episode 300000/300000: Average Reward (Last 10000 Episodes): 28.21\n"
     ]
    }
   ],
   "source": [
    "# In your main function:\n",
    "if __name__ == \"__main__\":\n",
    "    # Global variable to hold the Q-table\n",
    "    q_table_global = None\n",
    "    n = 300000\n",
    "\n",
    "    # Initialize the environment and Q-table for training\n",
    "    env_dynamic = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=4, num_resources=3, initial_energy=20, dynamic=True)\n",
    "    env_static = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=4, num_resources=2, initial_energy=20, dynamic=False)\n",
    "    \n",
    "    # Initialize the Q-table (pass the action space size from the environment)\n",
    "    q_table = initialize_q_table_dict(env_static.action_space)  # Use static env's action space\n",
    "\n",
    "    # Train the agent in the static environment\n",
    "    print(\"Training the Q-learning agent in the static environment...\")\n",
    "    static_rewards = q_learning_train_dict(env_static, q_table, episodes=n, max_steps=100)\n",
    "\n",
    "    # Train the agent in the dynamic environment\n",
    "    print(\"Training the Q-learning agent in the dynamic environment...\")\n",
    "    dynamic_rewards = q_learning_train_dict(env_dynamic, q_table, episodes=n, max_steps=100)\n",
    "\n",
    "    # Store the trained Q-table in a global variable\n",
    "    q_table_global = q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff1ea7-4931-4a47-93b4-99a10954e886",
   "metadata": {},
   "source": [
    "### Testing the Agent in Different Environments\n",
    "\n",
    "To better understand the agent's behavior, we will test it in both a **dynamic** and a **static** environment, using the **pre-trained Q-table**. This will help visualize how the agent moves and makes decisions based on the learned Q-values.\n",
    "\n",
    "In both environments, we can observe the agent’s movements and decisions, providing a clear understanding of its behavior. By testing the agent in these environments, we can see how it navigates, adapts, and applies its learning. This gives valuable insight into how Q-learning works in different contexts, whether the environment is stable or changes dynamically over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac88077c-e32a-4499-8aea-de06f4d298c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the agent in the dynamic environment:\n",
      "\n",
      "Testing the agent:\n",
      "D . . . . . . .\n",
      ". # . S . . . .\n",
      ". # S . . . . #\n",
      "# . . . . S . S\n",
      ". . . . R . . .\n",
      ". . . . . . . .\n",
      ". R . . . . . .\n",
      "# R . . . . . .\n",
      "Energy: 20\n",
      "\n",
      "Q-Values: [-1.42583566 12.21212334 -2.01170586  8.64395492]\n",
      "\n",
      "Step 1:\n",
      "Energy: 20\n",
      "Surrounding Information (around): (-1, 0, -1, 0)\n",
      "State: (20, (-1, 0, -1, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". S . . . . . R\n",
      "D # . . . . . .\n",
      ". # S . . . . #\n",
      "# . . . . . # .\n",
      ". . . . R . . .\n",
      ". . . . . . . .\n",
      ". R . . S . . .\n",
      "# R . . . . . S\n",
      "Energy: 19\n",
      "\n",
      "Q-Values: [ 9.49186316  8.91595046 -2.68002658 -2.5357945 ]\n",
      "\n",
      "Step 2:\n",
      "Energy: 19\n",
      "Surrounding Information (around): (0, 0, -1, 1)\n",
      "State: (19, (0, 0, -1, 1))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      "D S . . . . . R\n",
      ". # . . . . . .\n",
      ". # S . . . . #\n",
      "# . . . . . # .\n",
      ". . . . R . . .\n",
      ". . . . . . . .\n",
      ". R . . S . . .\n",
      "# R . . . . . S\n",
      "Energy: 18\n",
      "\n",
      "Q-Values: [-0.38220488  8.79261418  0.07957802  3.62517259]\n",
      "\n",
      "Step 3:\n",
      "Energy: 18\n",
      "Surrounding Information (around): (-1, 0, -1, 2)\n",
      "State: (18, (-1, 0, -1, 2))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". S . . . . . R\n",
      "D # . . . . . .\n",
      ". # S . . . . #\n",
      "# . . . . . # .\n",
      ". . . . R . . .\n",
      ". . . . . . . .\n",
      ". R . . S . . .\n",
      "# R . . . . . S\n",
      "Energy: 17\n",
      "\n",
      "Q-Values: [ 7.73724267  9.4832583  -4.18171008 -4.35735518]\n",
      "\n",
      "Step 4:\n",
      "Energy: 17\n",
      "Surrounding Information (around): (0, 0, -1, 1)\n",
      "State: (17, (0, 0, -1, 1))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . . . S .\n",
      "D # . . S . . #\n",
      "# . . . . . # .\n",
      ". . . . R . S .\n",
      ". . . . . S . .\n",
      ". R . . . . . .\n",
      "# R . . . . . .\n",
      "Energy: 16\n",
      "\n",
      "Q-Values: [ 7.54715031 -1.24563869  0.         -0.4457884 ]\n",
      "\n",
      "Step 5:\n",
      "Energy: 16\n",
      "Surrounding Information (around): (0, 1, -1, 1)\n",
      "State: (16, (0, 1, -1, 1))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      "D # . . . . S .\n",
      ". # . . S . . #\n",
      "# . . . . . # .\n",
      ". . . . R . S .\n",
      ". . . . . S . .\n",
      ". R . . . . . .\n",
      "# R . . . . . .\n",
      "Energy: 15\n",
      "\n",
      "Q-Values: [ 8.42067905  5.12274515 -3.6775114  -4.17443321]\n",
      "\n",
      "Step 6:\n",
      "Energy: 15\n",
      "Surrounding Information (around): (0, 0, -1, 1)\n",
      "State: (15, (0, 0, -1, 1))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      "D . . . . . . R\n",
      ". # . . . . S .\n",
      ". # . . S . . #\n",
      "# . . . . . # .\n",
      ". . . . R . S .\n",
      ". # . . . S . .\n",
      ". R . . . . . .\n",
      "# R . . . . . .\n",
      "Energy: 14\n",
      "\n",
      "Q-Values: [-4.46563566  6.9729302  -4.65888412  7.72695772]\n",
      "\n",
      "Step 7:\n",
      "Energy: 14\n",
      "Surrounding Information (around): (-1, 0, -1, 0)\n",
      "State: (14, (-1, 0, -1, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". D . S . . . R\n",
      ". # . . . . . .\n",
      ". # . . . . . #\n",
      "# . . . S . # .\n",
      ". . . . R . . .\n",
      ". # . . . . S .\n",
      ". R . . . . . .\n",
      "# R . S . . . .\n",
      "Energy: 13\n",
      "\n",
      "Q-Values: [-4.59678317 -4.61610209  6.17656186  7.65339296]\n",
      "\n",
      "Step 8:\n",
      "Energy: 13\n",
      "Surrounding Information (around): (-1, 1, 0, 0)\n",
      "State: (13, (-1, 1, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . D S . . . R\n",
      ". # . . . . . .\n",
      ". # . . . . . #\n",
      "# . . . S . # .\n",
      ". . . . R . . .\n",
      ". # . . . . S .\n",
      ". R . . . . . .\n",
      "# R . S R . . .\n",
      "Energy: 12\n",
      "\n",
      "Q-Values: [-0.62167882  6.05649746  5.61164182 15.65127464]\n",
      "\n",
      "Step 9:\n",
      "Energy: 12\n",
      "Surrounding Information (around): (-1, 0, 0, 2)\n",
      "State: (12, (-1, 0, 0, 2))\n",
      "Action Taken: 3\n",
      "Reward: 9\n",
      ". . . D . . . R\n",
      ". # . . . . . .\n",
      ". # . . . . . #\n",
      "# . . . S . # .\n",
      ". . . . R . . .\n",
      ". # . . . . S .\n",
      ". R . . . . . .\n",
      "# R . S R . . .\n",
      "Energy: 11\n",
      "\n",
      "Q-Values: [-5.09732533  7.12856339  5.45143769  6.63775552]\n",
      "\n",
      "Step 10:\n",
      "Energy: 11\n",
      "Surrounding Information (around): (-1, 0, 0, 0)\n",
      "State: (11, (-1, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". S . . . . . R\n",
      ". # . D . . . .\n",
      ". # . . . . . #\n",
      "# . . . . S # .\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R . . . . . .\n",
      "# R . . R . . S\n",
      "Energy: 10\n",
      "\n",
      "Q-Values: [4.90932985 6.84716527 5.466451   5.50912938]\n",
      "\n",
      "Step 11:\n",
      "Energy: 10\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (10, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". S . . . . . R\n",
      ". # . . . . . .\n",
      ". # . D . . . #\n",
      "# . . . . S # .\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R . . . . . .\n",
      "# R . . R . # S\n",
      "Energy: 9\n",
      "\n",
      "Q-Values: [5.38140936 5.75397209 5.3361632  5.83753537]\n",
      "\n",
      "Step 12:\n",
      "Energy: 9\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (9, (0, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". S . . . . . R\n",
      ". # . . . . . .\n",
      ". # . . D . . #\n",
      "# . . . . S # .\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R . . . . . .\n",
      "# R . . R . # S\n",
      "Energy: 8\n",
      "\n",
      "Q-Values: [4.65445476 5.58083801 5.30405952 4.69264175]\n",
      "\n",
      "Step 13:\n",
      "Energy: 8\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (8, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . . . . .\n",
      ". # . . . . . #\n",
      "# . S . D . # .\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R S . . . . S\n",
      "# R . . R . # .\n",
      "Energy: 7\n",
      "\n",
      "Q-Values: [ 2.82945989  6.18410112  2.1510428  11.44030151]\n",
      "\n",
      "Step 14:\n",
      "Energy: 7\n",
      "Surrounding Information (around): (0, 3, 0, 2)\n",
      "State: (7, (0, 3, 0, 2))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . . . . .\n",
      ". # . . . . . #\n",
      "# . S . . D # .\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R S . . . . S\n",
      "# R . . R . # .\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [ 4.24712819  3.33877366  3.40869604 -7.64203748]\n",
      "\n",
      "Step 15:\n",
      "Energy: 6\n",
      "Surrounding Information (around): (0, 0, 0, 1)\n",
      "State: (6, (0, 0, 0, 1))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . . R . .\n",
      ". # . . . D . #\n",
      "# . S . . . # .\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R S . . . . S\n",
      "# R . . R . # .\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [3.63274795 2.66957724 2.41988694 2.14794914]\n",
      "\n",
      "Step 16:\n",
      "Energy: 5\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (5, (0, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 4\n",
      ". . . . . . . R\n",
      ". # . . . D . .\n",
      ". # . . S . . #\n",
      "# . S . . . # #\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      "S R . . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 9\n",
      "\n",
      "Q-Values: [5.38140936 5.75397209 5.3361632  5.83753537]\n",
      "\n",
      "Step 17:\n",
      "Energy: 9\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (9, (0, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . . . D .\n",
      ". # . . S . . #\n",
      "# . S . . . # #\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      "S R . . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 8\n",
      "\n",
      "Q-Values: [4.65445476 5.58083801 5.30405952 4.69264175]\n",
      "\n",
      "Step 18:\n",
      "Energy: 8\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (8, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . . . . .\n",
      ". # . . S . D #\n",
      "# . S . . . # #\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      "S R . . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 7\n",
      "\n",
      "Q-Values: [ 4.36351844 -7.15120836  6.47442764 -7.40912856]\n",
      "\n",
      "Step 19:\n",
      "Energy: 7\n",
      "Surrounding Information (around): (0, 1, 0, 1)\n",
      "State: (7, (0, 1, 0, 1))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . S . . .\n",
      ". # . . . D . #\n",
      "# . . . . . # #\n",
      ". . . . R . . .\n",
      "S # . . . . . .\n",
      ". R . S . . . .\n",
      "# R . . R . # .\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [ 4.39055486  3.92467492 12.03800117  4.36556439]\n",
      "\n",
      "Step 20:\n",
      "Energy: 6\n",
      "Surrounding Information (around): (0, 0, 2, 0)\n",
      "State: (6, (0, 0, 2, 0))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . . . . . R\n",
      ". # . . S . . .\n",
      ". # . . D . . #\n",
      "# . . . . . # #\n",
      ". . . . R . . .\n",
      "S # . . . . . .\n",
      ". R . S . . . .\n",
      "# R . . R . # .\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [11.09484641  3.08808602  3.40907754  3.74535975]\n",
      "\n",
      "Step 21:\n",
      "Energy: 5\n",
      "Surrounding Information (around): (2, 0, 0, 0)\n",
      "State: (5, (2, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 9\n",
      ". . . . . . . R\n",
      ". # . . D . . .\n",
      ". # . . . . . #\n",
      "# . . . . . # #\n",
      ". . . . R . . .\n",
      "S # . . . . . .\n",
      ". R # S . . . .\n",
      "# R . . R . # .\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [3.32172819 2.28255477 2.10300061 2.15508179]\n",
      "\n",
      "Step 22:\n",
      "Energy: 4\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (4, (0, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . . D . . R\n",
      ". # . . . . . .\n",
      ". # . . . R . #\n",
      "# . . . . . # #\n",
      ". . . S R S . .\n",
      ". # . . . . . .\n",
      ". R # . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 3\n",
      "\n",
      "Q-Values: [-9.89405635  1.10888133  1.12095466  1.55503056]\n",
      "\n",
      "Step 23:\n",
      "Energy: 3\n",
      "Surrounding Information (around): (-1, 0, 0, 0)\n",
      "State: (3, (-1, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . . . D . R\n",
      ". # . . . . . .\n",
      ". # . . . R . #\n",
      "# . . . . . # #\n",
      ". . . S R S . .\n",
      ". # . . . . . .\n",
      ". R # . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 2\n",
      "\n",
      "Q-Values: [-10.87706081   0.10881265   0.53446792   0.58010203]\n",
      "\n",
      "Step 24:\n",
      "Energy: 2\n",
      "Surrounding Information (around): (-1, 0, 0, 0)\n",
      "State: (2, (-1, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . . . . D R\n",
      ". # . . . . . .\n",
      ". # . . . R . #\n",
      "# . . . . . # #\n",
      ". . . S R S . .\n",
      ". # . . . . . .\n",
      ". R # . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 1\n",
      "\n",
      "Q-Values: [-7.8932751   0.          0.          6.50927163]\n",
      "\n",
      "Step 25:\n",
      "Energy: 1\n",
      "Surrounding Information (around): (-1, 0, 0, 3)\n",
      "State: (1, (-1, 0, 0, 3))\n",
      "Action Taken: 3\n",
      "Reward: 4\n",
      ". . . . . . . D\n",
      ". # . . . . . .\n",
      ". # . . . R S #\n",
      "# . . . . . # #\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R # . . . S .\n",
      "# R . . R . # .\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [-8.78233051  0.27205881  2.69227102 -8.90162099]\n",
      "\n",
      "Step 26:\n",
      "Energy: 5\n",
      "Surrounding Information (around): (-1, 0, 0, -1)\n",
      "State: (5, (-1, 0, 0, -1))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . . . . D .\n",
      ". # . . . . . .\n",
      ". # . . # R S #\n",
      "# . . . . . # #\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R # . . . S .\n",
      "# R . . R . # .\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [-9.43735633  2.30154416  1.89491973  1.91219587]\n",
      "\n",
      "Step 27:\n",
      "Energy: 4\n",
      "Surrounding Information (around): (-1, 0, 0, 0)\n",
      "State: (4, (-1, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . D .\n",
      ". # . . # R S #\n",
      "# . . . . . # #\n",
      ". . . . R . . .\n",
      ". # . . . . . .\n",
      ". R # . . . S .\n",
      "# R . . R . # .\n",
      "Energy: 3\n",
      "\n",
      "Q-Values: [ 1.90709624 10.31203724  2.35996161  2.16667893]\n",
      "\n",
      "Step 28:\n",
      "Energy: 3\n",
      "Surrounding Information (around): (0, 2, 0, 0)\n",
      "State: (3, (0, 2, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 9\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # R D #\n",
      "# . . . . . # #\n",
      ". . . . R . . .\n",
      ". # S . . . . .\n",
      ". R # . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 2\n",
      "\n",
      "Q-Values: [ 0.01825851 -2.18848292  8.78203385 -1.56917276]\n",
      "\n",
      "Step 29:\n",
      "Energy: 2\n",
      "Surrounding Information (around): (0, 1, 3, 1)\n",
      "State: (2, (0, 1, 3, 1))\n",
      "Action Taken: 2\n",
      "Reward: 4\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # D . #\n",
      "# . . . . . # #\n",
      ". . . . R . . R\n",
      ". # S . . . . .\n",
      ". R # . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [ 3.38721615  5.58298082 -7.50695204  3.13709286]\n",
      "\n",
      "Step 30:\n",
      "Energy: 6\n",
      "Surrounding Information (around): (0, 0, 1, 0)\n",
      "State: (6, (0, 0, 1, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # . . #\n",
      "# . . . . D # #\n",
      ". . . . R . . R\n",
      ". # S . . . . .\n",
      ". R # . . . . .\n",
      "# R . . R . # .\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [ 2.19060777  3.03437854  2.56833115 -7.53514764]\n",
      "\n",
      "Step 31:\n",
      "Energy: 5\n",
      "Surrounding Information (around): (0, 0, 0, 1)\n",
      "State: (5, (0, 0, 0, 1))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . S . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . R D . R\n",
      ". # . . . . . .\n",
      ". R # . . # . .\n",
      "# R . . R . # .\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [ 3.6324253   3.45582742 10.43029129  4.0182792 ]\n",
      "\n",
      "Step 32:\n",
      "Energy: 4\n",
      "Surrounding Information (around): (0, 0, 3, 0)\n",
      "State: (4, (0, 0, 3, 0))\n",
      "Action Taken: 2\n",
      "Reward: 4\n",
      ". . . . . . . .\n",
      ". # . . S . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . D . . R\n",
      ". # . . . . . .\n",
      ". R # . . # . .\n",
      "# R . . R . # .\n",
      "Energy: 8\n",
      "\n",
      "Q-Values: [4.65445476 5.58083801 5.30405952 4.69264175]\n",
      "\n",
      "Step 33:\n",
      "Energy: 8\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (8, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . S . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . . . . R\n",
      ". # . . D . . .\n",
      ". R # . . # . .\n",
      "# R . . R . # .\n",
      "Energy: 7\n",
      "\n",
      "Q-Values: [4.07021429 3.93520263 4.73730873 4.03758783]\n",
      "\n",
      "Step 34:\n",
      "Energy: 7\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (7, (0, 0, 0, 0))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . . . . R\n",
      ". # . D . . . .\n",
      "S R # . . # . .\n",
      "# R . . R . # .\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [3.36768191 4.08326439 3.46487614 3.48160218]\n",
      "\n",
      "Step 35:\n",
      "Energy: 6\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (6, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . . . . R\n",
      ". # . . . . . .\n",
      "S R # D . # . .\n",
      "# R . . R . # .\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [ 2.13867341  4.5045336  -8.08665741  2.99912355]\n",
      "\n",
      "Step 36:\n",
      "Energy: 5\n",
      "Surrounding Information (around): (0, 0, 1, 0)\n",
      "State: (5, (0, 0, 1, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . # . R R\n",
      ". # . . . . . .\n",
      "S R # . . # . .\n",
      "# R . D R . # .\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [ 2.27085581 -3.64967033  1.26041053  9.09249613]\n",
      "\n",
      "Step 37:\n",
      "Energy: 4\n",
      "Surrounding Information (around): (0, -1, 0, 3)\n",
      "State: (4, (0, -1, 0, 3))\n",
      "Action Taken: 3\n",
      "Reward: 4\n",
      ". . . . . . . .\n",
      ". # . . S . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . # . R R\n",
      ". # . . . . . .\n",
      ". R # . . # . .\n",
      "# R . . D . # .\n",
      "Energy: 8\n",
      "\n",
      "Q-Values: [ 5.12211946 -6.55713846  4.23325822  4.00406689]\n",
      "\n",
      "Step 38:\n",
      "Energy: 8\n",
      "Surrounding Information (around): (0, -1, 0, 0)\n",
      "State: (8, (0, -1, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . S . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . # . R R\n",
      ". # . . . . . .\n",
      ". R # . D # . .\n",
      "# R . . . . # .\n",
      "Energy: 7\n",
      "\n",
      "Q-Values: [ 5.38298983  4.5095211   4.19254995 -6.95715391]\n",
      "\n",
      "Step 39:\n",
      "Energy: 7\n",
      "Surrounding Information (around): (0, 0, 0, 1)\n",
      "State: (7, (0, 0, 0, 1))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . S . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . # . R R\n",
      ". # . . D . . .\n",
      ". R # . . # . .\n",
      "# R . . . . # .\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [-7.70590265  3.29622918  3.4712583   3.7579539 ]\n",
      "\n",
      "Step 40:\n",
      "Energy: 6\n",
      "Surrounding Information (around): (1, 0, 0, 0)\n",
      "State: (6, (1, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . . # . R R\n",
      ". # . . . D . .\n",
      ". R # . . # . .\n",
      "# R . . . . # S\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [ 2.16874913 -8.2676186   2.85121952  0.13900664]\n",
      "\n",
      "Step 41:\n",
      "Energy: 5\n",
      "Surrounding Information (around): (0, 1, 0, 0)\n",
      "State: (5, (0, 1, 0, 0))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . # # . R R\n",
      ". # . . D . . .\n",
      ". R # . . # . .\n",
      "# R . . . . # S\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [-8.67600339  2.15933819  2.84208576  1.92528951]\n",
      "\n",
      "Step 42:\n",
      "Energy: 4\n",
      "Surrounding Information (around): (1, 0, 0, 0)\n",
      "State: (4, (1, 0, 0, 0))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      ". # . . . . . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . # # . R R\n",
      ". # . D . . . .\n",
      ". R # . . # . .\n",
      "# R . . . . # S\n",
      "Energy: 3\n",
      "\n",
      "Q-Values: [-9.30435255  1.51140355  1.29727087  2.34659148]\n",
      "\n",
      "Step 43:\n",
      "Energy: 3\n",
      "Surrounding Information (around): (1, 0, 0, 0)\n",
      "State: (3, (1, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      "S # . . . R . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . # # . R R\n",
      ". # . . D . . .\n",
      ". R # . . # . .\n",
      "# R . . . . # .\n",
      "Energy: 2\n",
      "\n",
      "Q-Values: [-10.60050731   1.39377468   0.35653542   0.37203762]\n",
      "\n",
      "Step 44:\n",
      "Energy: 2\n",
      "Surrounding Information (around): (1, 0, 0, 0)\n",
      "State: (2, (1, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      "S # . . . R . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . # # . R R\n",
      ". # . . . . . .\n",
      ". R # . D # . .\n",
      "# R . . . . # .\n",
      "Energy: 1\n",
      "\n",
      "Q-Values: [ 4.36223253e-07  1.36423079e-10  4.36223254e-07 -1.10000000e+01]\n",
      "\n",
      "Step 45:\n",
      "Energy: 1\n",
      "Surrounding Information (around): (0, 0, 0, 1)\n",
      "State: (1, (0, 0, 0, 1))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . . . . . .\n",
      "S # . . . R . .\n",
      ". # . . # . . #\n",
      "# . . . . . # #\n",
      ". . . # # . R R\n",
      ". # . . . . . .\n",
      ". R # D . # . .\n",
      "# R . . . . # .\n",
      "Energy: 0\n",
      "\n",
      "\n",
      "Episode finished after 45 steps with total reward 47\n",
      "\n",
      "Testing the agent in the static environment:\n",
      "\n",
      "Testing the agent:\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . R . . . . .\n",
      ". . . . . # . .\n",
      ". # D . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 20\n",
      "\n",
      "Q-Values: [ 8.99745582  9.88686981 -0.03865144 11.4648272 ]\n",
      "\n",
      "Step 1:\n",
      "Energy: 20\n",
      "Surrounding Information (around): (0, 0, 1, 0)\n",
      "State: (20, (0, 0, 1, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . R . . . . .\n",
      ". . . . . # . .\n",
      ". # . D . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 19\n",
      "\n",
      "Q-Values: [ 9.99829937  9.65367901  9.67126977 10.07318664]\n",
      "\n",
      "Step 2:\n",
      "Energy: 19\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (19, (0, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . R . . . . .\n",
      ". . . . . # . .\n",
      ". # . . D . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 18\n",
      "\n",
      "Q-Values: [9.56192144 9.39772031 9.54677691 9.45969472]\n",
      "\n",
      "Step 3:\n",
      "Energy: 18\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (18, (0, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . R . . . . .\n",
      ". . . . D # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 17\n",
      "\n",
      "Q-Values: [10.04384385  9.50548078  8.62013548 -1.85737513]\n",
      "\n",
      "Step 4:\n",
      "Energy: 17\n",
      "Surrounding Information (around): (0, 0, 0, 1)\n",
      "State: (17, (0, 0, 0, 1))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . R . D . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 16\n",
      "\n",
      "Q-Values: [8.33134693 8.56820393 8.98705411 8.03048895]\n",
      "\n",
      "Step 5:\n",
      "Energy: 16\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (16, (0, 0, 0, 0))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . R D . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 15\n",
      "\n",
      "Q-Values: [ 9.17164021  7.98566727 14.20574863  9.59164157]\n",
      "\n",
      "Step 6:\n",
      "Energy: 15\n",
      "Surrounding Information (around): (0, 0, 3, 0)\n",
      "State: (15, (0, 0, 3, 0))\n",
      "Action Taken: 2\n",
      "Reward: 4\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . D . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 19\n",
      "\n",
      "Q-Values: [ 9.99829937  9.65367901  9.67126977 10.07318664]\n",
      "\n",
      "Step 7:\n",
      "Energy: 19\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (19, (0, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . . . . R S\n",
      ". . . D . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 18\n",
      "\n",
      "Q-Values: [9.56192144 9.39772031 9.54677691 9.45969472]\n",
      "\n",
      "Step 8:\n",
      "Energy: 18\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (18, (0, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S . . . . .\n",
      "# . . D . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 17\n",
      "\n",
      "Q-Values: [9.948666   9.0875485  9.09978917 9.11322703]\n",
      "\n",
      "Step 9:\n",
      "Energy: 17\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (17, (0, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . S D . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 16\n",
      "\n",
      "Q-Values: [ 1.77204593  8.05595288 16.40792657  7.85993517]\n",
      "\n",
      "Step 10:\n",
      "Energy: 16\n",
      "Surrounding Information (around): (1, 0, 2, 0)\n",
      "State: (16, (1, 0, 2, 0))\n",
      "Action Taken: 2\n",
      "Reward: 9\n",
      ". . . # . S . .\n",
      ". . D . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 15\n",
      "\n",
      "Q-Values: [7.58894216 8.41618671 7.87664576 7.03870229]\n",
      "\n",
      "Step 11:\n",
      "Energy: 15\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (15, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . D . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 14\n",
      "\n",
      "Q-Values: [8.32509393 8.8440596  8.44554322 8.03391248]\n",
      "\n",
      "Step 12:\n",
      "Energy: 14\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (14, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . D . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 13\n",
      "\n",
      "Q-Values: [7.65269991 7.36735842 7.31451058 8.39802527]\n",
      "\n",
      "Step 13:\n",
      "Energy: 13\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (13, (0, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . D . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 12\n",
      "\n",
      "Q-Values: [6.64880462 6.7124063  6.67184497 7.31895176]\n",
      "\n",
      "Step 14:\n",
      "Energy: 12\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (12, (0, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . D . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 11\n",
      "\n",
      "Q-Values: [6.27045625 7.68864124 6.46507675 6.69365792]\n",
      "\n",
      "Step 15:\n",
      "Energy: 11\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (11, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . D # . .\n",
      ". # . . . . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 10\n",
      "\n",
      "Q-Values: [ 4.78329082  7.12898148  5.35273095 -4.65566611]\n",
      "\n",
      "Step 16:\n",
      "Energy: 10\n",
      "Surrounding Information (around): (0, 0, 0, 1)\n",
      "State: (10, (0, 0, 0, 1))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . D . . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 9\n",
      "\n",
      "Q-Values: [5.38140936 5.75397209 5.3361632  5.83753537]\n",
      "\n",
      "Step 17:\n",
      "Energy: 9\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (9, (0, 0, 0, 0))\n",
      "Action Taken: 3\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . D . .\n",
      ". . . . . S . .\n",
      ". . . # . . . .\n",
      "Energy: 8\n",
      "\n",
      "Q-Values: [-0.99226325 14.01405144  5.48731858  4.39468681]\n",
      "\n",
      "Step 18:\n",
      "Energy: 8\n",
      "Surrounding Information (around): (1, 2, 0, 0)\n",
      "State: (8, (1, 2, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 9\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . D . .\n",
      ". . . # . . . .\n",
      "Energy: 7\n",
      "\n",
      "Q-Values: [4.07021429 3.93520263 4.73730873 4.03758783]\n",
      "\n",
      "Step 19:\n",
      "Energy: 7\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (7, (0, 0, 0, 0))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . D . . .\n",
      ". . . # . . . .\n",
      "Energy: 6\n",
      "\n",
      "Q-Values: [3.36768191 4.08326439 3.46487614 3.48160218]\n",
      "\n",
      "Step 20:\n",
      "Energy: 6\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (6, (0, 0, 0, 0))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . . . . .\n",
      ". . . # D . . .\n",
      "Energy: 5\n",
      "\n",
      "Q-Values: [ 3.57180327 -8.36869795 -8.41900574  2.50027103]\n",
      "\n",
      "Step 21:\n",
      "Energy: 5\n",
      "Surrounding Information (around): (0, -1, 1, 0)\n",
      "State: (5, (0, -1, 1, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . . . . .\n",
      ". . . . D . . .\n",
      ". . . # . . . .\n",
      "Energy: 4\n",
      "\n",
      "Q-Values: [3.32172819 2.28255477 2.10300061 2.15508179]\n",
      "\n",
      "Step 22:\n",
      "Energy: 4\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (4, (0, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . D . . .\n",
      ". . . . . . . .\n",
      ". . . # . . . .\n",
      "Energy: 3\n",
      "\n",
      "Q-Values: [3.20814048 1.40786509 1.24104822 1.5558313 ]\n",
      "\n",
      "Step 23:\n",
      "Energy: 3\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (3, (0, 0, 0, 0))\n",
      "Action Taken: 0\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . D # . .\n",
      ". # . . . . . .\n",
      ". . . . . . . .\n",
      ". . . # . . . .\n",
      "Energy: 2\n",
      "\n",
      "Q-Values: [  0.51413822   0.68262085   0.29177746 -10.87497912]\n",
      "\n",
      "Step 24:\n",
      "Energy: 2\n",
      "Surrounding Information (around): (0, 0, 0, 1)\n",
      "State: (2, (0, 0, 0, 1))\n",
      "Action Taken: 1\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . . D . . .\n",
      ". . . . . . . .\n",
      ". . . # . . . .\n",
      "Energy: 1\n",
      "\n",
      "Q-Values: [5.80853089e-09 3.59858545e-29 6.41749210e-09 2.49801703e-09]\n",
      "\n",
      "Step 25:\n",
      "Energy: 1\n",
      "Surrounding Information (around): (0, 0, 0, 0)\n",
      "State: (1, (0, 0, 0, 0))\n",
      "Action Taken: 2\n",
      "Reward: 0\n",
      ". . . # . S . .\n",
      ". . . . . . . .\n",
      "# . . . . . R S\n",
      ". . . . . . . .\n",
      ". . . . . # . .\n",
      ". # . D . . . .\n",
      ". . . . . . . .\n",
      ". . . # . . . .\n",
      "Energy: 0\n",
      "\n",
      "\n",
      "Episode finished after 25 steps with total reward 22\n"
     ]
    }
   ],
   "source": [
    "env_dynamic2 = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=4, num_resources=3, initial_energy=20, dynamic=True)\n",
    "env_static2 = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=4, num_resources=2, initial_energy=20, dynamic=False)\n",
    "\n",
    "# Test the pre-trained agent in the dynamic environment\n",
    "print(\"\\nTesting the agent in the dynamic environment:\")\n",
    "test_pretrained_agent(env_dynamic2, q_table_global, max_steps=100)\n",
    "\n",
    "# Test the pre-trained agent in the static environment\n",
    "print(\"\\nTesting the agent in the static environment:\")\n",
    "test_pretrained_agent(env_static2, q_table_global, max_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a9364-58e3-4cc8-a47c-8307dc535362",
   "metadata": {},
   "source": [
    "### Running the Pre-trained Q-learning Agent\n",
    "\n",
    "As we trained the model and computed the Q-table in the previous steps, all you need to do now is utilize the computed `q_table_global`. \n",
    "\n",
    "The `q_learning_agent` function takes in a random environment and the pre-trained Q-table to calculate the total reward in that setting. It exploits the Q-table to decide the best actions for the agent based on its state, and returns the total reward accumulated over the episode.\n",
    "\n",
    "#### How to Use It:\n",
    "To run the agent, simply provide a random environment and the `q_table_global` like so:\n",
    "\n",
    "```python\n",
    "total_reward = q_learning_agent(env, q_table_global, max_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "594ee777-c184-484b-9a42-2e6ef173c2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Total Reward over 100 tests: 33.57\n"
     ]
    }
   ],
   "source": [
    "def q_learning_agent(env, q_table, max_steps=100):\n",
    "    \"\"\"\n",
    "    Runs a single episode using the pre-trained Q-table and returns the total reward.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Apply dynamic environment changes if applicable\n",
    "        if env.dynamic:\n",
    "            env.apply_dynamic_changes(step)\n",
    "\n",
    "        # Unpack the state after dynamic changes\n",
    "        energy, around = state[2], state[3]  # Extract energy and surrounding info from the state\n",
    "        state_key = compute_state_key(energy, around)  # Generate the state key\n",
    "\n",
    "        # Exploit the Q-table\n",
    "        if state_key in q_table:\n",
    "            action = np.argmax(q_table[state_key])\n",
    "        else:\n",
    "            # Handle unseen states (fallback to random action)\n",
    "            action = np.random.choice(list(env.action_space.keys()))\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Update state and accumulate reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # End the episode if done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_rewards = []  # List to store total rewards for each test\n",
    "\n",
    "    for i in range(100):  # Loop for 100 random environments\n",
    "        # Create a new random environment for each iteration\n",
    "        env = DisasterZoneEnv(width=8, height=8, num_obstacles=5, num_survivors=4, num_resources=2, initial_energy=20, dynamic=True)\n",
    "\n",
    "        # Get the total reward for the current environment\n",
    "        total_reward = q_learning_agent(env, q_table_global, max_steps=100)\n",
    "        total_rewards.append(total_reward)  # Store the total reward\n",
    "\n",
    "    # Compute the mean total reward over all tests\n",
    "    mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f\"\\nMean Total Reward over 100 tests: {mean_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
